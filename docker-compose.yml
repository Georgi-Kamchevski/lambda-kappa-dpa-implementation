version: '3.8'

services:
  
  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      POSTGRES_PASSWORD: "mysecretpassword"
      POSTGRES_DB: "thesisdb"
    ports:
      - "5432:5432"
    volumes:
      - ./postgres-data:/var/lib/postgresql/data
      - ./lambda_speed_schema.sql:/docker-entrypoint-initdb.d/lambda_speed_schema.sql
    networks:
      - kafka-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: '0.5'
        reservations:
          memory: 0.5g
          cpus: '0.25'

  prometheus:
    image: prom/prometheus
    ports: ["9090:9090"]
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/prometheus_data:/prometheus   
    networks: [kafka-net]
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: '0.5'

  grafana:
    image: grafana/grafana
    ports: ["3000:3000"]
    volumes:
      - ./monitoring/grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./monitoring/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
    networks: [kafka-net]
    depends_on: [prometheus]
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: '0.5'
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter
    environment:
      DATA_SOURCE_NAME: "postgresql://postgres:mysecretpassword@postgres:5432/thesisdb?sslmode=disable"
    ports: ["9187:9187"]
    networks: [kafka-net]
    deploy:
      resources:
        limits:
          memory: 0.5g
          cpus: '0.25'
  zookeeper:
    image: confluentinc/cp-zookeeper:6.2.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
    ports:
      - "2181:2181"
    networks:
      - kafka-net
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: '0.5'
  # 游릭 Kafka for Lambda
  kafka:
    image: confluentinc/cp-kafka:6.2.0
    container_name: kafka
    volumes:
      - ./monitoring/kafka-jmx:/opt/jmx_exporter
    depends_on: [zookeeper]
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,EXTERNAL://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,EXTERNAL://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_NUM_PARTITIONS: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_JMX_PORT: 9999
      KAFKA_OPTS: "-Dcom.sun.management.jmxremote \
        -Dcom.sun.management.jmxremote.authenticate=false \
        -Dcom.sun.management.jmxremote.ssl=false \
        -Djava.rmi.server.hostname=kafka \
        -Dcom.sun.management.jmxremote.port=9999 \
        -javaagent:/opt/jmx_exporter/jmx_prometheus_javaagent-0.16.1.jar=7071:/opt/jmx_exporter/kafka.yml"
    ports:
      - "9092:9092"
      - "29092:29092"
      - "7071:7071"
    networks: [kafka-net]
    profiles: ["lambda"]
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: '1.0'
        reservations:
          memory: 1g
          cpus: '0.5'
  # 游댯 Kafka for Kappa
  kafka-kappa:
    image: confluentinc/cp-kafka:6.2.0
    container_name: kafka-kappa
    volumes:
      - ./monitoring/kafka-jmx:/opt/jmx_exporter
    depends_on: [zookeeper]
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-kappa:9095, EXTERNAL://localhost:29095
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9095, EXTERNAL://0.0.0.0:29095
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_NUM_PARTITIONS: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_JMX_PORT: 9999
      KAFKA_OPTS: "-Dcom.sun.management.jmxremote \
                  -Dcom.sun.management.jmxremote.authenticate=false \
                  -Dcom.sun.management.jmxremote.ssl=false \
                  -Djava.rmi.server.hostname=kafka-kappa \
                  -Dcom.sun.management.jmxremote.port=9999 \
                  -Dcom.sun.management.jmxremote.rmi.port=9999 \
                  -javaagent:/opt/jmx_exporter/jmx_prometheus_javaagent-0.16.1.jar=7072:/opt/jmx_exporter/kafka.yml"
    ports:
      - "9095:9095"
      - "29095:29095"
      - "7072:7072"
    networks: [kafka-net]
    profiles: ["kappa"]
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: '1.0'
        reservations:
          memory: 1g
          cpus: '0.5'
  # 游릭 Spark Master for Lambda
  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    volumes:
      - ./:/opt/spark-apps
      - ./monitoring/spark-jmx:/opt/jmx
      - ./metrics.properties:/opt/bitnami/spark/conf/metrics.properties
    ports:
      - "8080:8080"
      - "7077:7077"
      - "7075:7075"  # Spark master JMX metrics port
    environment:
      - SPARK_MODE=master
      - SPARK_DAEMON_JAVA_OPTS=-javaagent:/opt/jmx/jmx_prometheus_javaagent-0.16.1.jar=7075:/opt/jmx/spark.yml
      - SPARK_SUBMIT_OPTIONS=--conf spark.metrics.conf.*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink

    depends_on: [kafka]
    networks: [kafka-net]
    profiles: ["lambda"]
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: '1.0'
        reservations:
          memory: 1g
          cpus: '0.5'
  spark-worker:
    image: bitnami/spark:3.5
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_DAEMON_JAVA_OPTS=-javaagent:/opt/jmx/jmx_prometheus_javaagent-0.16.1.jar=7075:/opt/jmx/spark.yml
      - SPARK_SUBMIT_OPTIONS=--conf spark.metrics.conf.*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink

    depends_on:
      - spark-master
    networks:
      - kafka-net
    profiles: ["lambda"]
    volumes:
      - ./:/opt/spark-apps
      - ./monitoring/spark-jmx:/opt/jmx
      - ./metrics.properties:/opt/bitnami/spark/conf/metrics.properties
    ports:
      - "7076:7075"  # Expose the worker JMX metrics port on a different host port (7076)
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: '2.0'
        reservations:
          memory: 1g
          cpus: '0.5'
  # 游릭 Flink JobManager for Lambda
  flink-jobmanager:
    image: apache/flink:1.18
    container_name: flink-jobmanager
    build:
      context: .
      dockerfile: dockerfile.flink
    ports:
      - "8081:8081"
      - "9250:9250"
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        jobmanager.memory.process.size: 2g
        jobmanager.classpaths: /opt/flink/custom-libs/*
        metrics.reporters: prom
        metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
        metrics.reporter.prom.port: 9250
        metrics.scope.jm: jobmanager
        metrics.scope.tm: taskmanager
        metrics.scope.job: jobmanager.<job_name>.<operator_name>
        metrics.scope.operator: taskmanager.<task_id>.<operator_name>
        metrics.system-resource: true
        metrics.job.numTasks: true
        metrics.job.task.start-delay: true
        metrics.job.task.idle-time: true
        metrics.job.task.network.input.buffer-pool-usage: true
        metrics.job.task.network.input.numBytesInLocal: true
        metrics.job.task.network.input.numBytesInRemote: true
        metrics.job.task.network.output.numBytesOut: true
        metrics.job.task.isBackPressured: true
        metrics.job.task.isBusyTimeMsPerSecond: true
        metrics.task.memory.used: true
        metrics.task.memory.total: true
        metrics.task.numRecordsIn: true
        metrics.task.numRecordsOut: true
        metrics.task.numBytesInLocal: true
        metrics.task.numBytesInRemote: true
        metrics.task.numBytesOut: true
        metrics.task.backPressureTimePerSecond: true
        metrics.managed.memory.used: true
        metrics.managed.memory.total: true
    volumes:
      - ./custom-flink-libs:/opt/flink/custom-libs
    networks: [kafka-net]
    profiles: ["lambda"]
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: '1.0'
    command: jobmanager
  # 游릭 Flink TaskManager for Lambda
  flink-taskmanager:
    image: apache/flink:1.18
    container_name: flink-taskmanager
    build:
      context: .
      dockerfile: dockerfile.flink
    ports:
      - "9251:9251"
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 1
        taskmanager.memory.process.size: 3g
        taskmanager.memory.flink.size: 2g
        taskmanager.memory.jvm-overhead: 512m
        taskmanager.heap.size: 512m
        taskmanager.memory.managed.size: 1g
        taskmanager.memory.network.size: 128m
        taskmanager.memory.framework.off-heap.size: 128m
        metrics.reporters: prom
        metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
        metrics.reporter.prom.port: 9251
        metrics.scope.jm: jobmanager
        metrics.scope.tm: taskmanager
        metrics.scope.job: jobmanager.<job_name>.<operator_name>
        metrics.scope.operator: taskmanager.<task_id>.<operator_name>
        metrics.system-resource: true
        metrics.job.numTasks: true
        metrics.job.task.start-delay: true
        metrics.job.task.idle-time: true
        metrics.job.task.network.input.buffer-pool-usage: true
        metrics.job.task.network.input.numBytesInLocal: true
        metrics.job.task.network.input.numBytesInRemote: true
        metrics.job.task.network.output.numBytesOut: true
        metrics.job.task.isBackPressured: true
        metrics.job.task.isBusyTimeMsPerSecond: true
        metrics.task.memory.used: true
        metrics.task.memory.total: true
        metrics.task.numRecordsIn: true
        metrics.task.numRecordsOut: true
        metrics.task.numBytesInLocal: true
        metrics.task.numBytesInRemote: true
        metrics.task.numBytesOut: true
        metrics.task.backPressureTimePerSecond: true
        metrics.managed.memory.used: true
        metrics.managed.memory.total: true
    volumes:
      - ./custom-flink-libs:/opt/flink/custom-libs
      - ./lambda_speed_layer.py:/opt/flink/usrlib/lambda_speed_layer.py
      - ./jdbc_test.py:/opt/flink/usrlib/jdbc_test.py
    depends_on: [flink-jobmanager]
    networks: [kafka-net]
    profiles: ["lambda"]
    deploy:
      resources:
        limits:
          memory: 8g
          cpus: '2.0'
    command: taskmanager
  # 游댯 Flink JobManager for Kappa
  flink-kappa-jobmanager:
    image: apache/flink:1.18
    container_name: flink-kappa-jobmanager
    build:
      context: .
      dockerfile: dockerfile.flink
    ports:
      - "8081:8081"
      - "9252:9252"
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-kappa-jobmanager
        jobmanager.memory.process.size: 2g
        jobmanager.classpaths: /opt/flink/custom-libs/*
        metrics.reporters: prom
        metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
        metrics.reporter.prom.port: 9252
        metrics.scope.jm: jobmanager
        metrics.scope.tm: taskmanager
        metrics.scope.job: jobmanager.<job_name>.<operator_name>
        metrics.scope.operator: taskmanager.<task_id>.<operator_name>
        metrics.system-resource: true
        metrics.job.numTasks: true
        metrics.job.task.start-delay: true
        metrics.job.task.idle-time: true
        metrics.job.task.network.input.buffer-pool-usage: true
        metrics.job.task.network.input.numBytesInLocal: true
        metrics.job.task.network.input.numBytesInRemote: true
        metrics.job.task.network.output.numBytesOut: true
        metrics.job.task.isBackPressured: true
        metrics.job.task.isBusyTimeMsPerSecond: true
        metrics.task.memory.used: true
        metrics.task.memory.total: true
        metrics.task.numRecordsIn: true
        metrics.task.numRecordsOut: true
        metrics.task.numBytesInLocal: true
        metrics.task.numBytesInRemote: true
        metrics.task.numBytesOut: true
        metrics.task.backPressureTimePerSecond: true
        metrics.managed.memory.used: true
        metrics.managed.memory.total: true
    volumes:
      - ./custom-flink-libs:/opt/flink/custom-libs
    networks: [kafka-net]
    profiles: ["kappa"]
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: '1.0'
    command: jobmanager
  # 游댯 Flink TaskManager for Kappa
  flink-kappa-taskmanager:
    image: apache/flink:1.18
    container_name: flink-kappa-taskmanager
    build:
      context: .
      dockerfile: dockerfile.flink
    ports:
      - "9253:9253"
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-kappa-jobmanager
        taskmanager.numberOfTaskSlots: 1
        taskmanager.memory.process.size: 3g
        taskmanager.memory.flink.size: 2g
        taskmanager.memory.jvm-overhead: 512m
        taskmanager.heap.size: 512m
        taskmanager.memory.managed.size: 1g
        taskmanager.memory.network.size: 128m
        taskmanager.memory.framework.off-heap.size: 128m
        metrics.reporters: prom
        metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
        metrics.reporter.prom.port: 9253
        metrics.scope.jm: jobmanager
        metrics.scope.tm: taskmanager
        metrics.scope.job: jobmanager.<job_name>.<operator_name>
        metrics.scope.operator: taskmanager.<task_id>.<operator_name>
        metrics.system-resource: true
        metrics.job.numTasks: true
        metrics.job.task.start-delay: true
        metrics.job.task.idle-time: true
        metrics.job.task.network.input.buffer-pool-usage: true
        metrics.job.task.network.input.numBytesInLocal: true
        metrics.job.task.network.input.numBytesInRemote: true
        metrics.job.task.network.output.numBytesOut: true
        metrics.job.task.isBackPressured: true
        metrics.job.task.isBusyTimeMsPerSecond: true
        metrics.task.memory.used: true
        metrics.task.memory.total: true
        metrics.task.numRecordsIn: true
        metrics.task.numRecordsOut: true
        metrics.task.numBytesInLocal: true
        metrics.task.numBytesInRemote: true
        metrics.task.numBytesOut: true
        metrics.task.backPressureTimePerSecond: true
        metrics.managed.memory.used: true
        metrics.managed.memory.total: true
    volumes:
      - ./custom-flink-libs:/opt/flink/custom-libs
      - ./kappa_speed_layer.py:/opt/flink/usrlib/kappa_speed_layer.py
    depends_on: [flink-kappa-jobmanager]
    networks: [kafka-net]
    profiles: ["kappa"]
    deploy:
      resources:
        limits:
          memory: 8g
          cpus: '2.0'
    command: taskmanager

networks:
  kafka-net:
    external: true
    name: lambda-kappa-dpa-implementation_kafka-net
